---
title: "Milestone Report"
date: "23 June 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Abstract 
The goal of this report is to present the strategy to use a corpus of text in order to predict the next word in a sentence. 
This report is divided into three main parts:

1. Data acquisition 
2. Data analysis  
3. Data prediction 


## Corpus reading
First we load the libraries used for the text mining and the data manipulation
```{r, warning=FALSE, message=FALSE}
library("tm")
library(ggplot2)
library(reshape)
library(dplyr)
library(RWeka)
library(stringr)

#source("https://bioconductor.org/biocLite.R")
#biocLite("Rgraphviz")
library(Rgraphviz)

set.seed(12345)
```

Then we download the corpus. For this report we will only use a subset of the original data. This option could be switch off.
After that, the text  is read and place into a Corpus.

```{r, warning=FALSE, message=FALSE}
#download.file(url="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip", destfile = "Coursera-SwiftKey.zip")
unzip("Coursera-SwiftKey.zip")

useSubDataSet = TRUE
if(useSubDataSet){
    #create a sub dataset with the 200th first lines of each files
    system("mkdir ./final/en_US_200/")
    system("head -200 ./final/en_US/en_US.blogs.txt > ./final/en_US_200/en_US.blogs.txt_200")
    system("head -200 ./final/en_US/en_US.news.txt > ./final/en_US_200/en_US.news.txt_200")
    system("head -200 ./final/en_US/en_US.twitter.txt > ./final/en_US_200/en_US.twitter.txt_200")
    
    ovid <- Corpus(DirSource("./final/en_US_200/"), readerControl = list(language = "en", load = TRUE))
}else{
    ovid <- Corpus(DirSource("./final/en_US/"), readerControl = list(language = "en", load = TRUE))
}
```

## Corpus overview 
To look more closely at the corpus, we can count the numbers of words and characters.
```{r}
counts <- data.frame(row.names = names(ovid))
counts$words <- sapply(lapply(ovid, words),length)
counts$char <- sapply(ovid, nchar)
counts
```
To better understand the numbers we can plot them:
```{r}
ggplot( melt(cbind(source=row.names(counts), counts), id=c("source")) , aes(x=factor(source), y=value)) + 
    geom_bar(stat = "identity")+ 
    facet_wrap(~variable) +
    labs(y = "count", x = "source", title = "Count of words and characters per Corpus") 
```

## Corpus Analysis
The second part will be the analysis of the corpus.
First we will parse the document and count the terms. For this counting we remove the punctuation, the numbers, and the English stop words. 
```{r, warning=FALSE}
tdm <- TermDocumentMatrix(ovid, control = list(removePunctuation = TRUE, stopwords = TRUE, removeNumbers = TRUE,  stemming = TRUE))
```
We can now see the 25 most frequent words per document.
```{r}
inspect(tdm[findFreqTerms(tdm, 25, Inf), 1:3])
```

We can also visualize the correlations between terms of a term-document matrix. 
```{r}
plot(tdm)
```

##Prediction

###Tokenization 
The first step for the prediction will be to tokenise the corpus to understand the sequences of the words.
In order to save time in each tokenization we execute the previous transformation directly in the corpus. 
```{r, warning=FALSE, message=FALSE}
#cleaning the corpus 
ovid <- tm_map(ovid, PlainTextDocument)
ovid <- tm_map(ovid, tolower)
ovid <- tm_map(ovid, stripWhitespace)
ovid <- tm_map(ovid,removePunctuation)
ovid <- tm_map(ovid, removeNumbers)
ovid <- tm_map(ovid, removeWords,stopwords("english"))
#Vcropus is mandatory for the tokenization
ovid <- VCorpus(VectorSource(ovid))
```

Then we can create three n-gram, one containing two words, one three and the last one between 4 and 5.
```{r}
#bigram 
bigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm_bigram <- TermDocumentMatrix(ovid, control = list(tokenize = bigramTokenizer))

frequency_bigram <- data.frame(row.names = rownames(tdm_bigram))
frequency_bigram$words <- rownames(tdm_bigram)
frequency_bigram$frequency <- rowSums(as.matrix(tdm_bigram))
frequency_bigram$type <- "bigram"
frequency_bigram <- arrange(frequency_bigram, desc(frequency)) 

#trigram 
trigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
tdm_trigram <- TermDocumentMatrix(ovid, control = list(tokenize = trigramTokenizer))

frequency_trigram <- data.frame(row.names = rownames(tdm_trigram))
frequency_trigram$words <- rownames(tdm_trigram)
frequency_trigram$frequency <- rowSums(as.matrix(tdm_trigram))
frequency_trigram$type <- "trigram"
frequency_trigram <- arrange(frequency_trigram, desc(frequency)) 

#ngram
ngramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 5))
tdm_ngram <- TermDocumentMatrix(ovid, control = list(tokenize = ngramTokenizer))

frequency_ngram <- data.frame(row.names = rownames(tdm_ngram))
frequency_ngram$words <- rownames(tdm_ngram)
frequency_ngram$frequency <- rowSums(as.matrix(tdm_ngram))
frequency_ngram$type <- "ngram"
frequency_ngram <- arrange(frequency_ngram, desc(frequency)) 
```

After the set-up of the frequency data frame for each ngram we can plot the bigram and the trigram to better understand the data.
```{r}
ggplot(rbind(frequency_bigram[1:20,], frequency_trigram[1:20,]) , aes(x=reorder(words, frequency), y=frequency)) + 
    geom_bar(stat = "identity")+ 
     coord_flip() +
    facet_wrap(~type, scales = "free") +
    labs(y = "count", x = "ngrqm", title = "Frequency of the 20th more frequent bigram and trigram") 


```

###Model 
The last part consist of building a model used for the prediction.
This model will contain all the ngrqm frequencies for each ngrams. We will be able to use this model to predict:

- The second word knowing one word
- The third word knowing two words
- Etc.
```{r}
predict_model <- rbind(frequency_bigram, frequency_trigram, frequency_ngram)
predict_model <- cbind(predict_model, as.data.frame(str_split_fixed(predict_model[,1], pattern = " ", n = 5)))
predict_model <- select(predict_model, -words)
predict_model[sample(1:nrow(predict_model),10),]
```

